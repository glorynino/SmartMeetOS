from __future__ import annotations

import argparse
import json
import os
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any

from agents.chunk_extractor_node import (
    extract_facts_from_smart_chunk_via_langchain_tools,
)
from processing.smart_chunker_node import smart_chunk_transcript


def _state_dir() -> Path:
    return Path(os.environ.get("SMARTMEETOS_STATE_DIR", ".smartmeetos_state")).resolve()


def _write_jsonl(path: Path, row: dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")


MEETING_SOURCE_VALUES: dict[str, str] = {
    "google_meet": "Google Meet",
    "zoom": "Zoom",
    "microsoft_teams": "Microsoft Teams",
}


def run_transcript_to_db_jsonl(
    *,
    transcript_text: str,
    meeting_id: str,
    source: str,
    out_dir: Path,
    max_chars: int,
    overlap_chars: int,
    max_workers: int,
) -> tuple[Path, Path]:
    """Pipeline: Smart Chunker Node -> Chunk Extractor LLM Node per chunk -> JSONL.

    Output JSONL rows are shaped like the DB table `extracted_facts`:
    - meeting_id
    - source_chunk_id
    - speaker
    - fact_type
    - fact_content
    - certainty
    - group_label (null)
    - created_at

    Note: `source_chunk_id` is generated by the chunker node (uuid4 as string).
    You can insert transcript chunks using these IDs so FK relationships hold.
    """

    # Resolve MeetingSource enum value string (matches DB enum values).
    source_value = MEETING_SOURCE_VALUES.get(source.strip().lower())
    if not source_value:
        raise ValueError(f"Invalid source '{source}'. Use one of: {', '.join(MEETING_SOURCE_VALUES.keys())}")

    chunks = smart_chunk_transcript(
        transcript_text,
        meeting_id=meeting_id,
        source=source_value,
        max_chars=max_chars,
        overlap_chars=overlap_chars,
    )

    if max_workers <= 0:
        max_workers = 1

    def extract_one(chunk) -> tuple[int, dict[str, Any]]:
        resp = extract_facts_from_smart_chunk_via_langchain_tools(chunk, meeting_id=str(meeting_id))
        return chunk.chunk_index, resp

    # Collect per-chunk results in parallel.
    results: list[tuple[int, dict[str, Any]]] = []
    if chunks:
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futs = [ex.submit(extract_one, c) for c in chunks]
            for fut in as_completed(futs):
                results.append(fut.result())

    # Stable order by chunk_index
    results.sort(key=lambda t: t[0])

    out_dir.mkdir(parents=True, exist_ok=True)
    ts = int(time.time())
    transcript_chunks_path = out_dir / f"transcript_chunks_{ts}.jsonl"
    extracted_facts_path = out_dir / f"extracted_facts_{ts}.jsonl"

    # 1) Write transcript_chunks rows (DB-shaped)
    for c in sorted(chunks, key=lambda x: x.chunk_index):
        _write_jsonl(
            transcript_chunks_path,
            {
                "id": c.id,
                "meeting_id": c.meeting_id,
                "chunk_index": c.chunk_index,
                "date": c.date.isoformat(),
                "speaker": c.speaker,
                "chunk_content": c.chunk_content,
                "source": c.source,
            },
        )

    # 2) Write extracted_facts rows (DB-shaped)
    for _chunk_index, resp in results:
        for row in resp.get("facts", []) or []:
            _write_jsonl(extracted_facts_path, row)

    return transcript_chunks_path, extracted_facts_path


def main(argv: list[str] | None = None) -> int:
    p = argparse.ArgumentParser(description="Smart Chunker -> Chunk Extractor (Groq) -> DB-shaped JSONL.")
    p.add_argument("--input", required=True, help="Path to a UTF-8 transcript text file.")
    p.add_argument("--meeting-id", required=True, help="Meeting UUID (meetings.id). Required for DB inserts.")
    p.add_argument(
        "--source",
        default="google_meet",
        choices=sorted(MEETING_SOURCE_VALUES.keys()),
        help="Meeting source (maps to MeetingSource enum).",
    )
    p.add_argument("--max-chars", type=int, default=2000)
    p.add_argument("--overlap-chars", type=int, default=200)
    p.add_argument(
        "--max-workers",
        type=int,
        default=int(os.environ.get("EXTRACT_MAX_WORKERS", "4")),
        help="Max parallel chunk extraction workers (default: EXTRACT_MAX_WORKERS or 4).",
    )
    p.add_argument(
        "--out-dir",
        default=str(_state_dir() / "db_jsonl"),
        help="Output directory for DB-shaped JSONL files (default: SMARTMEETOS_STATE_DIR/db_jsonl).",
    )

    args = p.parse_args(argv)

    input_path = Path(args.input)
    transcript_text = input_path.read_text(encoding="utf-8")

    out_dir = Path(args.out_dir)

    transcript_chunks_path, extracted_facts_path = run_transcript_to_db_jsonl(
        transcript_text=transcript_text,
        meeting_id=args.meeting_id,
        source=args.source,
        out_dir=out_dir,
        max_chars=args.max_chars,
        overlap_chars=args.overlap_chars,
        max_workers=args.max_workers,
    )

    print(str(transcript_chunks_path))
    print(str(extracted_facts_path))
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
